{"cells":[{"cell_type":"markdown","metadata":{"id":"XGV-KL6pL1nE"},"source":["# SparkSession vs SparkContext"]},{"cell_type":"markdown","metadata":{"id":"_KYu3hrML1nG"},"source":["## Importamos SparkContext y SparkSession"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6T70QOZhL1nH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682503843446,"user_tz":-120,"elapsed":51753,"user":{"displayName":"Luis Sastre Roca","userId":"16085014804845980263"}},"outputId":"1b1bd2de-05a9-4484-90f5-e0bf570a2103"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install pyspark --quiet\n","from pyspark import SparkContext\n","from pyspark.sql import SparkSession"]},{"cell_type":"markdown","metadata":{"id":"chASV6-OL1nI"},"source":["## Creamos nuestra primera sesion"]},{"cell_type":"markdown","source":["En Apache Spark, una **sesión es una conexión de un cliente con el cluster Spark**. La sesión se utiliza para enviar trabajos al cluster y obtener resultados de ellos. Una sesión se puede iniciar de forma interactiva desde la línea de comandos o programáticamente desde una aplicación de Spark.\n","\n","Un **contexto de Spark** es un objeto que representa una conexión a un cluster Spark y proporciona un punto de acceso a todas las funcionalidades de Spark. En una aplicación de Spark, normalmente se crea un contexto al principio del programa y se utiliza para realizar todas las operaciones de Spark en esa aplicación. Por ejemplo, podrías utilizar un contexto para crear un conjunto de datos (RDD), aplicar transformaciones y acciones sobre él y obtener resultados.\n","\n","Es importante mencionar que **una sesión puede tener múltiples contextos**, y que cada contexto se puede utilizar para realizar operaciones de Spark de forma independiente. Por ejemplo, podrías tener un contexto para procesar datos en tiempo real y otro contexto para realizar análisis en lote."],"metadata":{"id":"WrkEikPTe4Bu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HKjXd8P6L1nI"},"outputs":[],"source":["#Convenio\n","#Sesión --> spark\n","#Contexto --> sc\n","\n","spark = SparkSession.builder \\\n","        .master(\"local\") \\\n","        .appName(\"miPrimerApplicacion\") \\\n","        .getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"F3614c6xL1nI"},"source":["## Terminamos la sesión actual\n","\n","No podemos tener mas de una sesión a la vez en nuestro notebook, por lo cual con el método 'stop' terminaremos la applicación.\n","\n","De la misma forma, al terminar una applicación, debemos de indicar explicitamente que termine. De otra forma no liberará los recursos asignados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3iVqN4pL1nJ"},"outputs":[],"source":["spark.stop()"]},{"cell_type":"markdown","metadata":{"id":"Dl01ZDfnL1nJ"},"source":["## Creamos una sesión heredando los atributos de un contexto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhQR5VG1L1nK"},"outputs":[],"source":["sc = SparkContext(master=\"local\",appName = \"miPrimerContexto\")\n","spark2 = SparkSession(sc)"]},{"cell_type":"code","source":["spark2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"8Gsb4FRtQhpo","executionInfo":{"status":"ok","timestamp":1682504024835,"user_tz":-120,"elapsed":2110,"user":{"displayName":"Luis Sastre Roca","userId":"16085014804845980263"}},"outputId":"0e813668-9840-4277-bb2c-936df8b47b91"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f3475fd05e0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://0ac604b613dd:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.4.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>miPrimerContexto</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"nD83MOvNL1nL"},"source":["## Creamos una sesión múltiple"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VO00IauNL1nL"},"outputs":[],"source":["sparkSession2 = spark2.newSession()"]},{"cell_type":"markdown","metadata":{"id":"2Hzhe1urL1nL"},"source":["## Revisamos que los tres objetos apuntan a la misma aplicación\n","\n","Aprovechando la salida que nos ofrece, conocemos SparkUI, el monitor por excelencia para Spark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QKY6nAKL1nM"},"outputs":[],"source":["spark2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehCs3bdfL1nN"},"outputs":[],"source":["sparkSession2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LAqMi_scL1nN"},"outputs":[],"source":["spark2.stop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"1FCXcJcTqXYBYBNmWDSH91I_wrcpjNCs-","timestamp":1682504412247},{"file_id":"https://github.com/terranigmark/curso-apache-spark-platzi/blob/master/1.%20Jupyter%20vs%20CLI.ipynb","timestamp":1672137901465}]}},"nbformat":4,"nbformat_minor":0}